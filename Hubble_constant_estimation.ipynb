{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee\n",
    "import corner\n",
    "from IPython.display import display, Math\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=False)\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets us first import the data and define the proper things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0708 0.09   0.12   0.17   0.179  0.199  0.2    0.24   0.27   0.28\n",
      " 0.35   0.352  0.38   0.3802 0.4    0.4004 0.4247 0.43   0.44   0.4497\n",
      " 0.47   0.4783 0.48   0.51   0.57   0.593  0.6    0.61   0.68   0.73\n",
      " 0.781  0.875  0.88   0.9    1.037  1.3    1.363  1.43   1.53   1.75\n",
      " 1.965  2.34   2.36  ]\n",
      "[ 69.    69.    68.6   83.    75.    75.    72.9   79.69  77.    88.8\n",
      "  84.4   83.    81.2   83.    95.    77.    87.1   86.45  82.6   92.8\n",
      "  89.    80.9   97.    90.9   92.4  104.    87.9   98.96  92.    97.3\n",
      " 105.   125.    90.   117.   154.   168.   160.   177.   140.   202.\n",
      " 186.5  222.   226.  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=np.loadtxt(\"OHD.txt\")\n",
    "z_data=data[:,0]\n",
    "H_data=data[:,1]\n",
    "H_err= data[:,2]\n",
    "errsq=1/H_err**2\n",
    "print(z_data)\n",
    "print(H_data)\n",
    "len(z_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first Friedmann equation is:\n",
    "\n",
    "$$\n",
    "H^2 \\equiv \\left(\\frac{\\dot{a}}{a}\\right)^2 = \\frac{8 \\pi G}{3} \\rho - \\frac{k}{a^2} + \\frac{\\Lambda}{3}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "* $a(t)$ is the scale factor, $H = \\dot{a}/a$\n",
    "* $\\rho$ is total energy density\n",
    "* $k$ is curvature ($k=0$ for flat)\n",
    "* $\\Lambda$ is the cosmological constant\n",
    "\n",
    "Define the **critical density** today:\n",
    "\n",
    "$$\n",
    "\\rho_c = \\frac{3 H_0^2}{8 \\pi G}\n",
    "$$\n",
    "\n",
    "and the **density parameters**:\n",
    "\n",
    "$$\n",
    "\\Omega_m = \\frac{\\rho_{m0}}{\\rho_c}, \\quad\n",
    "\\Omega_\\Lambda = \\frac{\\Lambda}{3 H_0^2}, \\quad\n",
    "\\Omega_k = -\\frac{k}{(a_0 H_0)^2}\n",
    "$$\n",
    "\n",
    "Rewriting Friedmann in terms of these:\n",
    "\n",
    "$$\n",
    "\\frac{H^2(a)}{H_0^2} = \\Omega_m a^{-3} + \\Omega_k a^{-2} + \\Omega_\\Lambda\n",
    "$$\n",
    "\n",
    "The scale factor relates to redshift as $a = \\frac{1}{1+z}$, so:\n",
    "\n",
    "$$\n",
    "H^2(z) = H_0^2 \\left[ \\Omega_m (1+z)^3 + \\Omega_k (1+z)^2 + \\Omega_\\Lambda \\right]\n",
    "$$\n",
    "\n",
    "For a **flat universe** ($\\Omega_k = 0$), $\\Omega_\\Lambda = 1 - \\Omega_m$, giving:\n",
    "\n",
    "$$\n",
    "H^2(z) = H_0^2 \\left[ \\Omega_m (1+z)^3 + (1 - \\Omega_m) \\right]\n",
    "$$\n",
    "\n",
    "Finally, taking the square root:\n",
    "\n",
    "$$\n",
    "H(z) = H_0 \\sqrt{\\Omega_m (1+z)^3 + (1 - \\Omega_m)}\n",
    "$$\n",
    "\n",
    "This is our model which we are trying to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hmodel(z, params):\n",
    "    om0, H0 = params\n",
    "    Hval=H0*np.sqrt(om0*(1+z)**3 + (1-om0))\n",
    "    return Hval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define the model parameters and returning the chisquared $\\newline$\n",
    " $\\chi^2 = [D-T]^T \\frac{1}{\\sigma^2} [D-T]$, where $\\sigma^2$ also shows the particular diagonal of covariance matrix $Cov^{-1}$ error on that value. Here $D$ shows the data and $T$ shows the theoretical model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chisq(D,T,err):\n",
    "    diff= D-T\n",
    "    chisq=np.sum(((D-T)/err)**2)\n",
    "    return chisq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the theoretical Hubble equation itself, we know that the matter density $\\Omega_m$ and Hubble constant $H_0$ can only take certain physically reasonable values — for example, $H_0$ can’t be negative and $\\Omega_m$ must be between 0 and 1. Observations from previous studies also give rough bounds on these parameters. These constraints are called **priors**: they encode what we already know and prevent our analysis from wandering into unrealistic regions, which saves time and ensures we’re exploring the parameter space correctly.\n",
    "\n",
    "The code then compares the theoretical Hubble parameter $H(z)$ to the actual measured data using a chi-squared statistic, which tells us how well a given choice of parameters fits the observations. This chi-squared is converted into a **likelihood**, and combining it with the priors gives the **posterior**, which tells us the probability distribution of the parameters given the data — basically, what values are most consistent with both theory and observations. In Bayesian inference, the **posterior** is the probability of parameters $\\theta$ given the data $D$, and is given by Bayes’ theorem:\n",
    "\n",
    "$$\n",
    "P(\\theta \\mid D) = \\frac{P(D \\mid \\theta) P(\\theta)}{P(D)}\n",
    "$$\n",
    "\n",
    "Here, $P(\\theta \\mid D)$ is the posterior, $P(D \\mid \\theta)$ is the likelihood, $P(\\theta)$ is the prior, and $P(D)$ is the evidence, which normalizes the distribution:\n",
    "\n",
    "$$\n",
    "P(D) = \\int P(D \\mid \\theta) P(\\theta) \\, d\\theta\n",
    "$$\n",
    "\n",
    "In our code, the priors will be implemented as uniform constraints:\n",
    "\n",
    "$$\n",
    "P(\\theta) =\n",
    "\\begin{cases} \n",
    "1 & \\text{if } 0 < \\Omega_m < 0.5 \\text{ and } 30 < H_0 < 100 \\\\[2mm]\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The likelihood assumes Gaussian errors on the Hubble data, so\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = P(D \\mid \\theta) \\propto \\exp\\Big[-\\frac{1}{2} \\chi^2(\\theta)\\Big]\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\chi^2(\\theta) = \\sum_i \\frac{\\big(H_{\\text{obs},i} - H_{\\text{model}}(z_i;\\theta)\\big)^2}{\\sigma_i^2}.\n",
    "$$\n",
    "\n",
    "In log space, which your code uses, the posterior becomes\n",
    "\n",
    "$$\n",
    "\\log P(\\theta \\mid D) = \\log \\mathcal{L}(\\theta) + \\log P(\\theta) + \\text{constant} = -\\frac{1}{2} \\chi^2(\\theta) + \\log P(\\theta) + \\text{const}.\n",
    "$$\n",
    "\n",
    "This is exactly what our `modelparameter` and `H_loglike` functions will implement: the chi-squared gives the likelihood, the prior is enforced via bounds, and together they define the posterior probability for $\\Omega_m$ and $H_0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelparameter(params):\n",
    "    om0, H0 = params\n",
    "    HT = np.zeros(len(z_data))\n",
    "    if not (30 < H0 < 100 and 0 < om0 <0.5):\n",
    "        return -np.inf\n",
    "      \n",
    "    for i in range(len(z_data)):\n",
    "        HT[i] = Hmodel(z_data[i], params)\n",
    "    Hubblechi = chisq(H_data, HT, H_err)\n",
    "    return Hubblechi\n",
    "        \n",
    "def H_loglike(params):\n",
    "    H_log = -0.5 * modelparameter(params)\n",
    "    # Ensure that log likelihood is finite\n",
    "    if not np.isfinite(H_log):\n",
    "        return -np.inf\n",
    "    return H_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined our model and the corresponding likelihood function, we now proceed to explore the parameter space using the **MCMC algorithm**. The basic idea is as follows:\n",
    "\n",
    "We first identify the **dimensionality of our parameter space** — in this case, two parameters, Ωₘ and H₀. We then initialize a set of **independent walkers**, each representing a separate chain that will traverse the parameter space. These walkers start at different positions to ensure a broad coverage of the posterior.\n",
    "\n",
    "At each iteration, every walker proposes a new position based on the ensemble of all other walkers, and this proposal is accepted or rejected according to the likelihood of the parameters. Over many iterations, the walkers collectively sample the **posterior distribution**, exploring high-probability regions more densely and low-probability regions less frequently.\n",
    "\n",
    "By **tracking the trajectory of each walker**, we can monitor convergence, identify burn-in periods, and eventually obtain a comprehensive set of samples representing the posterior. From these samples, we can estimate **best-fit parameter values**, uncertainties, and correlations between parameters, providing a complete statistical characterization of the model in light of the data.\n",
    "\n",
    "This approach allows us not only to determine the most probable values of the parameters but also to understand the **shape, spread, and degeneracies** in the parameter space, which is crucial for interpreting cosmological measurements reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 CPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:45<00:00, 110.09it/s]\n"
     ]
    }
   ],
   "source": [
    "n_iteration=5000\n",
    "nwalker=100\n",
    "ndim=2 #this dfines the number of model parameters that required to be constrained.\n",
    "\n",
    "#let us define the initial position of workers of mcmc\n",
    "init_position = np.random.uniform(low=[0.00001, 30.], high=[0.5, 99], size=(nwalker, ndim))\n",
    "ncpu = cpu_count()\n",
    "print(\"{0} CPUs\".format(ncpu))\n",
    "\n",
    "with Pool(processes=cpu_count()) as pool:\n",
    "    sampler = emcee.EnsembleSampler(nwalker, ndim, H_loglike, pool=pool)\n",
    "    sampler.run_mcmc(init_position, n_iteration, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an MCMC ensemble sampler, the chain is stored as a **3D array**:\n",
    "\n",
    "$$\n",
    "\\text{chain.shape} = (\\text{n\\_walkers}, \\text{n\\_steps}, \\text{n\\_dim})\n",
    "$$\n",
    "\n",
    "* **`n_walkers`** = number of walkers exploring the parameter space\n",
    "* **`n_steps`** = number of iterations/steps per walker\n",
    "* **`n_dim`** = number of parameters in the model\n",
    "\n",
    "So initially, the chain is a **matrix of size**:\n",
    "\n",
    "$$\n",
    "n_\\text{walkers} \\times n_\\text{steps} \\times n_\\text{dim}\n",
    "$$\n",
    "\n",
    "Each entry along the last axis stores the **numerical values of all parameters** for a given walker at a given step:\n",
    "\n",
    "$$\n",
    "\\text{chain[i, j, :]} = (\\theta_1, \\theta_2, \\dots, \\theta_{n_\\text{dim}})\n",
    "$$\n",
    "\n",
    "* `i` = walker index\n",
    "* `j` = iteration index\n",
    "* `:` = vector of parameter values in the model\n",
    "\n",
    "But cornor plot accepts a 2D array, therefore we need to flatten it. **Flattening** the chain means reshaping this 3D array into a 2D array of size:\n",
    "\n",
    "$$\n",
    "(n_\\text{walkers} \\cdot n_\\text{steps}) \\times n_\\text{dim}\n",
    "$$\n",
    "\n",
    "* Flattening **does not multiply indices arbitrarily**. Instead, it sequentially stacks the samples:\n",
    "\n",
    "$$\n",
    "k = i \\cdot n_\\text{steps} + j\n",
    "$$\n",
    "\n",
    "* `k` is the new single index in the flattened array\n",
    "* Each `(i,j)` pair maps to a **unique `k`**, so there’s no collision\n",
    "* The last axis (`n_dim`) remains unchanged, still storing the parameter vector\n",
    "\n",
    "* Example: `n_walkers=2, n_steps=3`\n",
    "\n",
    "| i | j | k = i\\*3 + j |\n",
    "| - | - | ------------ |\n",
    "| 0 | 0 | 0            |\n",
    "| 0 | 1 | 1            |\n",
    "| 0 | 2 | 2            |\n",
    "| 1 | 0 | 3            |\n",
    "| 1 | 1 | 4            |\n",
    "| 1 | 2 | 5            |\n",
    "\n",
    "No duplicates. ✅\n",
    "\n",
    "Once we have **flattened the MCMC chain** into a 2D array of shape `(n_walkers * n_steps, n_dim)`, each row represents a **sample of the full parameter vector** drawn from the posterior distribution. Essentially, this array tells us **where in parameter space the walkers spent their time** after the burn-in period — the denser the points in a region, the higher the posterior probability there.\n",
    "\n",
    "The next step is to **summarize this distribution** for each parameter in a meaningful way. Computing the **16th, 50th, and 84th percentiles** for each parameter gives us:\n",
    "\n",
    "* **Median (50th percentile):** The parameter value around which the walkers spent most of their time; effectively the “central” value of the posterior.\n",
    "* **16th and 84th percentiles:** A measure of the uncertainty or spread in the posterior. The difference between the median and these percentiles gives **asymmetric error bars**, reflecting the fact that the posterior is often **not perfectly symmetric**.\n",
    "\n",
    "So the percentile computation is motivated by the desire to **capture where the walkers “lived” in parameter space** and to **quantify the uncertainty in the parameter estimates** in a straightforward, robust way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\mathrm{\\Omega_M} = 0.26_{-0.02}^{+0.02}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\mathrm{H_0} = 70.58_{-1.47}^{+1.43}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Flatten chain: shape = (nwalker*niter, ndim)\n",
    "flat_samples = sampler.get_chain(flat=True)\n",
    "# Labels for parameters\n",
    "labels = [r'\\Omega_M', r'H_0']\n",
    "\n",
    "results = []\n",
    "for i in range(ndim):\n",
    "    # Compute percentiles for parameter i\n",
    "    p16, p50, p84 = np.percentile(flat_samples[:, i], [16, 50, 84])\n",
    "    \n",
    "    # Store results (median, +err, -err)\n",
    "    median = p50\n",
    "    plus_err = p84 - p50\n",
    "    minus_err = p50 - p16\n",
    "    results.append((median, plus_err, minus_err))\n",
    "\n",
    "    # Format result as LaTeX for display\n",
    "    txt = r\"\\mathrm{{{name}}} = {val:.2f}_{{-{low:.2f}}}^{{+{high:.2f}}}\".format(\n",
    "        name=labels[i],\n",
    "        val=median,\n",
    "        low=minus_err,\n",
    "        high=plus_err\n",
    "    )\n",
    "    display(Math(txt))\n",
    "\n",
    "# Unpack results for convenience\n",
    "om_mc, H0_mc = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MCMC, you have **walkers (or a chain)** exploring your parameter space. At the very beginning:\n",
    "\n",
    "* Your walkers are usually **initialized randomly**, often far from the high-probability region.\n",
    "* The first few steps are mostly “finding their way” toward the posterior peak.\n",
    "* These early samples **do not reflect the true posterior** — they’re biased by your starting positions.\n",
    "\n",
    "**Burn-in** = the process of **discarding these initial steps**. If you don’t discard burn-in:\n",
    "\n",
    "* Your posterior estimates (mean, median, percentiles) will be **biased** toward the starting positions.\n",
    "* Trace plots will look strange — the chains show an artificial trend at the beginning.\n",
    "* Corner plots will be distorted.\n",
    "\n",
    "### How to choose burn-in\n",
    "\n",
    "* **Rule of thumb:** discard \\~2–3 × **autocorrelation time** (`tau`) per parameter.\n",
    "* `tau` measures how many steps it takes for the chain to become statistically independent.\n",
    "\n",
    "Now that, using **burn-in**, we took care of the initial positions and allowed the walkers to reach the high-probability regions, there still remains one more issue. The fact that each next step a walker takes is **correlated with its previous step**—by design of the MCMC algorithm—introduces **autocorrelation** in the chain. This means consecutive samples are not fully independent, so if we naively treat them all as independent, we **overestimate the effective number of samples** and can underestimate uncertainties.\n",
    "\n",
    "To take care of this issue, we **thin the chain**: we only keep every N-th sample and discard the ones in between. This reduces the correlations between retained samples, giving us a set of points that are closer to being statistically independent, while still representing the posterior distribution. Example:\n",
    "\n",
    "* Original chain (values = iterations): `[0,1,2,3,4,5,6,7,8,9]`\n",
    "* Thin every 3rd sample → `[0,3,6,9]`\n",
    "\n",
    "With these caveats in mind, we will now proceed to plotting the figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocorrelation time per parameter: [29.44882463 28.20401772]\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ndim, figsize=(18,9), sharex=True)\n",
    "\n",
    "# chain shape: (nsteps, nwalkers, ndim)\n",
    "tau = sampler.get_autocorr_time()\n",
    "print(\"Autocorrelation time per parameter:\", tau)\n",
    "burnin = int(2 * np.max(tau))   # discard 2 × max autocorrelation\n",
    "thin = int(0.5 * np.min(tau))  # optional: thin by ~half the min autocorr\n",
    "\n",
    "params = [r'\\Omega_M', r'H_0']\n",
    "\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    # Plot each walker for parameter i\n",
    "    for walker in range(sampler.get_chain(discard=burnin, thin=thin).shape[1]):\n",
    "        ax.plot(sampler.get_chain(discard=burnin, thin=thin)[:, walker, i], color='black', alpha=0.1, lw=0.15)\n",
    "    ax.set_ylabel(f\"${params[i]}$\")\n",
    "    ax.set_xlabel(\"step number\")\n",
    "\n",
    "plt.tight_layout ()\n",
    "\n",
    "samples=sampler.get_chain(discard=burnin,thin=thin,flat=True)\n",
    "fig=corner.corner(samples,labels=[r'$\\Omega_M$', r'$H_0$'],show_titles=True,color='green',\n",
    "                  Truths=[om_mc,H0_mc],\n",
    "                  smooth=2.,\n",
    "                  Levels=[0.68,0.95],\n",
    "                  plot_density=True,\n",
    "                  plot_datapoints=False)\n",
    "\n",
    "fig.savefig(\"corner_plot.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
